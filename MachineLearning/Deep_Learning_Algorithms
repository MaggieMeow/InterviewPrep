# Deep Learning Algorithms

### Attention mechanism
Before Attention mechanism, translation relies on reading a complete sentence and compress all information into a fixed-length vector. This leads to information loss. Long range dependencies are required in RNN and is difficult to account for. No multi-step back propagation.

An attention function maps a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.

### Self-attention
Self-attention, or intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. 
 

### Transformer
the Transformer is a transduction model. It relies entirely n an attention mechanism to draw global dependencies between input and output without using sequence aligned RNNs or convolution. This allows more parallisation.

### Deep Averaging Network


 
 
### Word2vec
a group of related models that are used to produce word embeddings.

### Word embedding
Word embedding maps words or phrases from the vocabulary to vectors of real numbers. 

### Neural Turing machine

### Transduction
Or transduction inference is reasoning from observed, specific (training) cases to specific (test) cases. In contrast to induction which reasons from observed training cases to general rules.

### Autoregressive model
The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term).

### ResNet
